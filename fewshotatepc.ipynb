{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bin/sh: nvidia-smi: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUDA GPU found in your device\n",
      "Version 0.9.10.post1 of metric_visualizer is outdated. Version 0.9.13.post1 was released Wednesday May 01, 2024.\n",
      "[2024-06-06 01:09:57] (2.3.1) \u001b[31mPyABSA(2.3.1): If your code crashes on Colab, please use the GPU runtime. Then run \"pip install pyabsa[dev] -U\" and restart the kernel.\n",
      "Or if it does not work, you can use v1.x versions, e.g., pip install pyabsa<2.0 -U\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "WARNING: When you fails to load a checkpoint, e.g., Unexpected key(s),\n",
      "Try to downgrade transformers<=4.29.0.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dehan/opt/anaconda3/lib/python3.9/multiprocessing/pool.py:265: ResourceWarning: unclosed running multiprocessing pool <multiprocessing.pool.Pool state=RUN pool_size=1>\n",
      "  _warn(f\"unclosed running multiprocessing pool {self!r}\",\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "from pyabsa import AspectTermExtraction as ATEPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = (\n",
    "    ATEPC.ATEPCConfigManager.get_atepc_config_english()\n",
    ")  # this config contains 'pretrained_bert', it is based on pretrained models\n",
    "config.model = ATEPC.ATEPCModelList.FAST_LCF_ATEPC  # improved version of LCF-ATEPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyabsa import DatasetItem\n",
    "my_dataset = DatasetItem(\"my_dataset\", [\"132.manuallabel\"])\n",
    "# my_dataset1 and my_dataset2 are the dataset folders. In there folders, the train dataset is necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-06-06 01:11:32] (2.3.1) Set Model Device: cpu\n",
      "[2024-06-06 01:11:32] (2.3.1) Device Name: Unknown\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024-06-06 01:11:33,178 INFO: PyABSA version: 2.3.1\n",
      "2024-06-06 01:11:33,179 INFO: Transformers version: 4.29.0\n",
      "2024-06-06 01:11:33,179 INFO: Torch version: 2.1.0.dev20230313+cudaNone\n",
      "2024-06-06 01:11:33,180 INFO: Device: Unknown\n",
      "2024-06-06 01:11:33,180 INFO: Scenario Case: manuallabelcheckpointoriginal\n",
      "2024-06-06 01:11:33,180 INFO: my_dataset in the trainer is not a exact path, will search dataset in current working directory\n",
      "2024-06-06 01:11:33,212 INFO: You can set load_aug=True in a trainer to augment your dataset (English only yet) and improve performance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/Users/dehan/opt/anaconda3/lib/python3.9/multiprocessing/pool.py:265: ResourceWarning: unclosed running multiprocessing pool <multiprocessing.pool.Pool state=RUN pool_size=1>\n",
      "  _warn(f\"unclosed running multiprocessing pool {self!r}\",\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/Users/dehan/opt/anaconda3/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "convert examples to features: 100%|██████████| 1072/1072 [00:02<00:00, 441.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-06 01:11:38,086 INFO: Dataset Label Details: {'Neutral': 10, 'Positive': 752, 'Negative': 310, 'Sum': 1072}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "convert examples to features: 100%|██████████| 358/358 [00:00<00:00, 454.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-06 01:11:39,194 INFO: Dataset Label Details: {'Neutral': 4, 'Positive': 248, 'Negative': 106, 'Sum': 358}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.classifier.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-06 01:11:41,612 INFO: Save cache dataset to fast_lcf_atepc.my_dataset.dataset.1b5394f81d6c24a085e3b6a0477f2827e8dfbc9baebd3e061bdc0e12274aa68f.cache\n",
      "2024-06-06 01:11:41,651 INFO: ABSADatasetsVersion:None\t-->\tCalling Count:0\n",
      "2024-06-06 01:11:41,652 INFO: IOB_label_to_index:{'B-ASP': 1, 'I-ASP': 2, 'O': 3, '[CLS]': 4, '[SEP]': 5}\t-->\tCalling Count:2\n",
      "2024-06-06 01:11:41,652 INFO: MV:<metric_visualizer.metric_visualizer.MetricVisualizer object at 0x172b48f10>\t-->\tCalling Count:6\n",
      "2024-06-06 01:11:41,652 INFO: PyABSAVersion:2.3.1\t-->\tCalling Count:2\n",
      "2024-06-06 01:11:41,653 INFO: SRD:3\t-->\tCalling Count:5720\n",
      "2024-06-06 01:11:41,653 INFO: TorchVersion:2.1.0.dev20230313+cudaNone\t-->\tCalling Count:2\n",
      "2024-06-06 01:11:41,653 INFO: TransformersVersion:4.29.0\t-->\tCalling Count:2\n",
      "2024-06-06 01:11:41,654 INFO: auto_device:True\t-->\tCalling Count:7\n",
      "2024-06-06 01:11:41,654 INFO: batch_size:16\t-->\tCalling Count:10\n",
      "2024-06-06 01:11:41,654 INFO: cache_dataset:True\t-->\tCalling Count:2\n",
      "2024-06-06 01:11:41,655 INFO: checkpoint_save_mode:1\t-->\tCalling Count:8\n",
      "2024-06-06 01:11:41,655 INFO: cross_validate_fold:-1\t-->\tCalling Count:2\n",
      "2024-06-06 01:11:41,656 INFO: dataset_file:{'train': ['integrated_datasets/atepc_datasets/132.manuallabel/train_set.txt.atepc'], 'test': ['integrated_datasets/atepc_datasets/132.manuallabel/test_set.txt.atepc'], 'valid': []}\t-->\tCalling Count:12\n",
      "2024-06-06 01:11:41,656 INFO: dataset_name:my_dataset\t-->\tCalling Count:10\n",
      "2024-06-06 01:11:41,659 INFO: device:cpu\t-->\tCalling Count:218\n",
      "2024-06-06 01:11:41,660 INFO: device_name:Unknown\t-->\tCalling Count:2\n",
      "2024-06-06 01:11:41,660 INFO: dropout:0.5\t-->\tCalling Count:2\n",
      "2024-06-06 01:11:41,661 INFO: dynamic_truncate:True\t-->\tCalling Count:5720\n",
      "2024-06-06 01:11:41,663 INFO: embed_dim:768\t-->\tCalling Count:0\n",
      "2024-06-06 01:11:41,666 INFO: evaluate_begin:0\t-->\tCalling Count:1\n",
      "2024-06-06 01:11:41,666 INFO: from_checkpoint:/Users/dehan/Documents/SKRIPSIII/PyABSAfarrel/checkpoints/ENGLISH_ATE_TOURISM\t-->\tCalling Count:3\n",
      "2024-06-06 01:11:41,667 INFO: gradient_accumulation_steps:1\t-->\tCalling Count:6\n",
      "2024-06-06 01:11:41,668 INFO: hidden_dim:768\t-->\tCalling Count:12\n",
      "2024-06-06 01:11:41,670 INFO: index_to_IOB_label:{1: 'B-ASP', 2: 'I-ASP', 3: 'O', 4: '[CLS]', 5: '[SEP]'}\t-->\tCalling Count:0\n",
      "2024-06-06 01:11:41,672 INFO: index_to_label:{0: 'Negative', 1: 'Neutral', 2: 'Positive'}\t-->\tCalling Count:4\n",
      "2024-06-06 01:11:41,675 INFO: inference_model:None\t-->\tCalling Count:0\n",
      "2024-06-06 01:11:41,677 INFO: initializer:xavier_uniform_\t-->\tCalling Count:0\n",
      "2024-06-06 01:11:41,683 INFO: l2reg:1e-05\t-->\tCalling Count:4\n",
      "2024-06-06 01:11:41,685 INFO: label_list:['B-ASP', 'I-ASP', 'O', '[CLS]', '[SEP]']\t-->\tCalling Count:3\n",
      "2024-06-06 01:11:41,685 INFO: label_to_index:{'Negative': 0, 'Neutral': 1, 'Positive': 2}\t-->\tCalling Count:0\n",
      "2024-06-06 01:11:41,686 INFO: lcf:cdm\t-->\tCalling Count:23\n",
      "2024-06-06 01:11:41,686 INFO: learning_rate:2e-05\t-->\tCalling Count:2\n",
      "2024-06-06 01:11:41,687 INFO: load_aug:False\t-->\tCalling Count:2\n",
      "2024-06-06 01:11:41,687 INFO: log_step:50\t-->\tCalling Count:1\n",
      "2024-06-06 01:11:41,687 INFO: logger:<Logger fast_lcf_atepc (INFO)>\t-->\tCalling Count:22\n",
      "2024-06-06 01:11:41,688 INFO: max_seq_len:80\t-->\tCalling Count:20070\n",
      "2024-06-06 01:11:41,688 INFO: max_test_metrics:{'max_apc_test_acc': 0, 'max_apc_test_f1': 0, 'max_apc_test_precision': 0, 'max_apc_test_recall': 0, 'max_ate_test_f1': 0}\t-->\tCalling Count:5\n",
      "2024-06-06 01:11:41,689 INFO: metrics_of_this_checkpoint:{'apc_acc': 0, 'apc_f1': 0, 'apc_precision': 0, 'apc_recall': 0, 'ate_f1': 0}\t-->\tCalling Count:0\n",
      "2024-06-06 01:11:41,689 INFO: model:<class 'pyabsa.tasks.AspectTermExtraction.models.__lcf__.fast_lcf_atepc.FAST_LCF_ATEPC'>\t-->\tCalling Count:8\n",
      "2024-06-06 01:11:41,690 INFO: model_name:fast_lcf_atepc\t-->\tCalling Count:2870\n",
      "2024-06-06 01:11:41,690 INFO: model_path_to_save:checkpoints\t-->\tCalling Count:5\n",
      "2024-06-06 01:11:41,690 INFO: num_epoch:1\t-->\tCalling Count:3\n",
      "2024-06-06 01:11:41,690 INFO: num_labels:6\t-->\tCalling Count:4\n",
      "2024-06-06 01:11:41,691 INFO: optimizer:adamw\t-->\tCalling Count:4\n",
      "2024-06-06 01:11:41,691 INFO: output_dim:3\t-->\tCalling Count:5\n",
      "2024-06-06 01:11:41,691 INFO: overwrite_cache:False\t-->\tCalling Count:0\n",
      "2024-06-06 01:11:41,692 INFO: path_to_save:None\t-->\tCalling Count:2\n",
      "2024-06-06 01:11:41,692 INFO: patience:99999\t-->\tCalling Count:1\n",
      "2024-06-06 01:11:41,693 INFO: pretrained_bert:microsoft/deberta-v3-base\t-->\tCalling Count:15\n",
      "2024-06-06 01:11:41,694 INFO: save_mode:1\t-->\tCalling Count:0\n",
      "2024-06-06 01:11:41,698 INFO: scenario_case:manuallabelcheckpointoriginal\t-->\tCalling Count:2\n",
      "2024-06-06 01:11:41,698 INFO: seed:52\t-->\tCalling Count:14\n",
      "2024-06-06 01:11:41,699 INFO: sep_indices:2\t-->\tCalling Count:716\n",
      "2024-06-06 01:11:41,699 INFO: show_metric:False\t-->\tCalling Count:0\n",
      "2024-06-06 01:11:41,699 INFO: spacy_model:en_core_web_sm\t-->\tCalling Count:7\n",
      "2024-06-06 01:11:41,699 INFO: srd_alignment:True\t-->\tCalling Count:0\n",
      "2024-06-06 01:11:41,700 INFO: task_code:ATEPC\t-->\tCalling Count:2\n",
      "2024-06-06 01:11:41,700 INFO: task_name:Aspect Term Extraction and Polarity Classification\t-->\tCalling Count:1\n",
      "2024-06-06 01:11:41,701 INFO: tokenizer:DebertaV2TokenizerFast(name_or_path='microsoft/deberta-v3-base', vocab_size=128000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)\t-->\tCalling Count:0\n",
      "2024-06-06 01:11:41,701 INFO: use_amp:False\t-->\tCalling Count:2\n",
      "2024-06-06 01:11:41,702 INFO: use_bert_spc:True\t-->\tCalling Count:46\n",
      "2024-06-06 01:11:41,702 INFO: use_syntax_based_SRD:False\t-->\tCalling Count:2860\n",
      "2024-06-06 01:11:41,703 INFO: warmup_step:-1\t-->\tCalling Count:2\n",
      "2024-06-06 01:11:41,703 INFO: window:lr\t-->\tCalling Count:0\n",
      "2024-06-06 01:11:41,705 INFO: Model Architecture:\n",
      " FAST_LCF_ATEPC(\n",
      "  (bert4global): DebertaV2Model(\n",
      "    (embeddings): DebertaV2Embeddings(\n",
      "      (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "      (dropout): StableDropout()\n",
      "    )\n",
      "    (encoder): DebertaV2Encoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x DebertaV2Layer(\n",
      "          (attention): DebertaV2Attention(\n",
      "            (self): DisentangledSelfAttention(\n",
      "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (pos_dropout): StableDropout()\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "            (output): DebertaV2SelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): DebertaV2Intermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): DebertaV2Output(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "            (dropout): StableDropout()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (rel_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (SA1): Encoder(\n",
      "    (encoder): ModuleList(\n",
      "      (0): SelfAttention(\n",
      "        (SA): BertSelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (tanh): Tanh()\n",
      "  )\n",
      "  (SA2): Encoder(\n",
      "    (encoder): ModuleList(\n",
      "      (0): SelfAttention(\n",
      "        (SA): BertSelfAttention(\n",
      "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (tanh): Tanh()\n",
      "  )\n",
      "  (linear_double): Linear(in_features=1536, out_features=768, bias=True)\n",
      "  (linear_triple): Linear(in_features=2304, out_features=768, bias=True)\n",
      "  (pooler): BertPooler(\n",
      "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (activation): Tanh()\n",
      "  )\n",
      "  (dense): Linear(in_features=768, out_features=3, bias=True)\n",
      "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
      ")\n",
      "2024-06-06 01:11:41,705 INFO: ABSADatasetsVersion:None\t-->\tCalling Count:0\n",
      "2024-06-06 01:11:41,706 INFO: IOB_label_to_index:{'B-ASP': 1, 'I-ASP': 2, 'O': 3, '[CLS]': 4, '[SEP]': 5}\t-->\tCalling Count:2\n",
      "2024-06-06 01:11:41,706 INFO: MV:<metric_visualizer.metric_visualizer.MetricVisualizer object at 0x172b48f10>\t-->\tCalling Count:6\n",
      "2024-06-06 01:11:41,706 INFO: PyABSAVersion:2.3.1\t-->\tCalling Count:2\n",
      "2024-06-06 01:11:41,706 INFO: SRD:3\t-->\tCalling Count:5720\n",
      "2024-06-06 01:11:41,707 INFO: TorchVersion:2.1.0.dev20230313+cudaNone\t-->\tCalling Count:2\n",
      "2024-06-06 01:11:41,707 INFO: TransformersVersion:4.29.0\t-->\tCalling Count:2\n",
      "2024-06-06 01:11:41,707 INFO: auto_device:True\t-->\tCalling Count:8\n",
      "2024-06-06 01:11:41,707 INFO: batch_size:16\t-->\tCalling Count:10\n",
      "2024-06-06 01:11:41,707 INFO: cache_dataset:True\t-->\tCalling Count:2\n",
      "2024-06-06 01:11:41,708 INFO: checkpoint_save_mode:1\t-->\tCalling Count:8\n",
      "2024-06-06 01:11:41,708 INFO: cross_validate_fold:-1\t-->\tCalling Count:3\n",
      "2024-06-06 01:11:41,708 INFO: dataset_file:{'train': ['integrated_datasets/atepc_datasets/132.manuallabel/train_set.txt.atepc'], 'test': ['integrated_datasets/atepc_datasets/132.manuallabel/test_set.txt.atepc'], 'valid': []}\t-->\tCalling Count:12\n",
      "2024-06-06 01:11:41,708 INFO: dataset_name:my_dataset\t-->\tCalling Count:10\n",
      "2024-06-06 01:11:41,708 INFO: device:cpu\t-->\tCalling Count:221\n",
      "2024-06-06 01:11:41,709 INFO: device_name:Unknown\t-->\tCalling Count:2\n",
      "2024-06-06 01:11:41,709 INFO: dropout:0.5\t-->\tCalling Count:2\n",
      "2024-06-06 01:11:41,709 INFO: dynamic_truncate:True\t-->\tCalling Count:5720\n",
      "2024-06-06 01:11:41,709 INFO: embed_dim:768\t-->\tCalling Count:0\n",
      "2024-06-06 01:11:41,709 INFO: evaluate_begin:0\t-->\tCalling Count:1\n",
      "2024-06-06 01:11:41,710 INFO: from_checkpoint:/Users/dehan/Documents/SKRIPSIII/PyABSAfarrel/checkpoints/ENGLISH_ATE_TOURISM\t-->\tCalling Count:3\n",
      "2024-06-06 01:11:41,710 INFO: gradient_accumulation_steps:1\t-->\tCalling Count:6\n",
      "2024-06-06 01:11:41,710 INFO: hidden_dim:768\t-->\tCalling Count:12\n",
      "2024-06-06 01:11:41,710 INFO: index_to_IOB_label:{1: 'B-ASP', 2: 'I-ASP', 3: 'O', 4: '[CLS]', 5: '[SEP]'}\t-->\tCalling Count:0\n",
      "2024-06-06 01:11:41,711 INFO: index_to_label:{0: 'Negative', 1: 'Neutral', 2: 'Positive'}\t-->\tCalling Count:4\n",
      "2024-06-06 01:11:41,711 INFO: inference_model:None\t-->\tCalling Count:0\n",
      "2024-06-06 01:11:41,711 INFO: initializer:xavier_uniform_\t-->\tCalling Count:0\n",
      "2024-06-06 01:11:41,712 INFO: l2reg:1e-05\t-->\tCalling Count:4\n",
      "2024-06-06 01:11:41,712 INFO: label_list:['B-ASP', 'I-ASP', 'O', '[CLS]', '[SEP]']\t-->\tCalling Count:3\n",
      "2024-06-06 01:11:41,712 INFO: label_to_index:{'Negative': 0, 'Neutral': 1, 'Positive': 2}\t-->\tCalling Count:0\n",
      "2024-06-06 01:11:41,712 INFO: lcf:cdm\t-->\tCalling Count:23\n",
      "2024-06-06 01:11:41,713 INFO: learning_rate:2e-05\t-->\tCalling Count:2\n",
      "2024-06-06 01:11:41,713 INFO: load_aug:False\t-->\tCalling Count:2\n",
      "2024-06-06 01:11:41,713 INFO: log_step:50\t-->\tCalling Count:1\n",
      "2024-06-06 01:11:41,713 INFO: logger:<Logger fast_lcf_atepc (INFO)>\t-->\tCalling Count:23\n",
      "2024-06-06 01:11:41,714 INFO: max_seq_len:80\t-->\tCalling Count:20070\n",
      "2024-06-06 01:11:41,714 INFO: max_test_metrics:{'max_apc_test_acc': 0, 'max_apc_test_f1': 0, 'max_apc_test_precision': 0, 'max_apc_test_recall': 0, 'max_ate_test_f1': 0}\t-->\tCalling Count:5\n",
      "2024-06-06 01:11:41,714 INFO: metrics_of_this_checkpoint:{'apc_acc': 0, 'apc_f1': 0, 'apc_precision': 0, 'apc_recall': 0, 'ate_f1': 0}\t-->\tCalling Count:0\n",
      "2024-06-06 01:11:41,715 INFO: model:<class 'pyabsa.tasks.AspectTermExtraction.models.__lcf__.fast_lcf_atepc.FAST_LCF_ATEPC'>\t-->\tCalling Count:8\n",
      "2024-06-06 01:11:41,715 INFO: model_name:fast_lcf_atepc\t-->\tCalling Count:2870\n",
      "2024-06-06 01:11:41,715 INFO: model_path_to_save:checkpoints\t-->\tCalling Count:5\n",
      "2024-06-06 01:11:41,715 INFO: num_epoch:1\t-->\tCalling Count:3\n",
      "2024-06-06 01:11:41,715 INFO: num_labels:6\t-->\tCalling Count:4\n",
      "2024-06-06 01:11:41,716 INFO: optimizer:adamw\t-->\tCalling Count:4\n",
      "2024-06-06 01:11:41,716 INFO: output_dim:3\t-->\tCalling Count:5\n",
      "2024-06-06 01:11:41,716 INFO: overwrite_cache:False\t-->\tCalling Count:0\n",
      "2024-06-06 01:11:41,716 INFO: path_to_save:None\t-->\tCalling Count:2\n",
      "2024-06-06 01:11:41,717 INFO: patience:99999\t-->\tCalling Count:1\n",
      "2024-06-06 01:11:41,717 INFO: pretrained_bert:microsoft/deberta-v3-base\t-->\tCalling Count:15\n",
      "2024-06-06 01:11:41,717 INFO: save_mode:1\t-->\tCalling Count:0\n",
      "2024-06-06 01:11:41,717 INFO: scenario_case:manuallabelcheckpointoriginal\t-->\tCalling Count:2\n",
      "2024-06-06 01:11:41,717 INFO: seed:52\t-->\tCalling Count:14\n",
      "2024-06-06 01:11:41,718 INFO: sep_indices:2\t-->\tCalling Count:716\n",
      "2024-06-06 01:11:41,718 INFO: show_metric:False\t-->\tCalling Count:0\n",
      "2024-06-06 01:11:41,718 INFO: spacy_model:en_core_web_sm\t-->\tCalling Count:7\n",
      "2024-06-06 01:11:41,718 INFO: srd_alignment:True\t-->\tCalling Count:0\n",
      "2024-06-06 01:11:41,719 INFO: task_code:ATEPC\t-->\tCalling Count:2\n",
      "2024-06-06 01:11:41,719 INFO: task_name:Aspect Term Extraction and Polarity Classification\t-->\tCalling Count:1\n",
      "2024-06-06 01:11:41,719 INFO: tokenizer:DebertaV2TokenizerFast(name_or_path='microsoft/deberta-v3-base', vocab_size=128000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)\t-->\tCalling Count:0\n",
      "2024-06-06 01:11:41,720 INFO: use_amp:False\t-->\tCalling Count:2\n",
      "2024-06-06 01:11:41,720 INFO: use_bert_spc:True\t-->\tCalling Count:46\n",
      "2024-06-06 01:11:41,720 INFO: use_syntax_based_SRD:False\t-->\tCalling Count:2860\n",
      "2024-06-06 01:11:41,720 INFO: warmup_step:-1\t-->\tCalling Count:2\n",
      "2024-06-06 01:11:41,720 INFO: window:lr\t-->\tCalling Count:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dehan/Documents/SKRIPSIII/PyABSAfarrel/pyabsa/framework/instructor_class/instructor_template.py:435: ResourceWarning: unclosed file <_io.BufferedReader name='checkpoints/ENGLISH_ATE_TOURISM/fast_lcf_atepc.config'>\n",
      "  config = pickle.load(open(config_path[0], \"rb\"))\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-06 01:11:42,219 INFO: Resume trainer from Checkpoint: /Users/dehan/Documents/SKRIPSIII/PyABSAfarrel/checkpoints/ENGLISH_ATE_TOURISM!\n",
      "2024-06-06 01:11:42,282 INFO: ***** Running training for Aspect Term Extraction and Polarity Classification *****\n",
      "2024-06-06 01:11:42,283 INFO:   Num examples = 1072\n",
      "2024-06-06 01:11:42,283 INFO:   Batch size = 16\n",
      "2024-06-06 01:11:42,284 INFO:   Num steps = 67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/67 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 14\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# config.batch_size = 16\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# config.patience = 2\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# config.log_step = -1\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# config.seed = [1]\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# config.verbose = False  # If verbose == True, PyABSA will output the model strcture and seversal processed data examples\u001b[39;00m\n\u001b[1;32m     12\u001b[0m config\u001b[38;5;241m.\u001b[39mscenario_case\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmanuallabelcheckpointoriginal\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 14\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mATEPC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mATEPCTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmy_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrom_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/Users/dehan/Documents/SKRIPSIII/PyABSAfarrel/checkpoints/ENGLISH_ATE_TOURISM\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# if you want to resume training from our pretrained checkpoints, you can pass the checkpoint name here\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauto_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDeviceTypeOption\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAUTO\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# use cuda if available\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_save_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mModelSaveOption\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSAVE_MODEL_STATE_DICT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# save state dict only instead of the whole model\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_aug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# there are some augmentation dataset for integrated datasets, you use them by setting load_aug=True to improve performance\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/SKRIPSIII/PyABSAfarrel/pyabsa/tasks/AspectTermExtraction/trainer/atepc_trainer.py:69\u001b[0m, in \u001b[0;36mATEPCTrainer.__init__\u001b[0;34m(self, config, dataset, from_checkpoint, checkpoint_save_mode, auto_device, path_to_save, load_aug)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtask_code \u001b[38;5;241m=\u001b[39m TaskCodeOption\u001b[38;5;241m.\u001b[39mAspect_Term_Extraction_and_Classification\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtask_name \u001b[38;5;241m=\u001b[39m TaskNameOption()\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m     66\u001b[0m     TaskCodeOption\u001b[38;5;241m.\u001b[39mAspect_Term_Extraction_and_Classification\n\u001b[1;32m     67\u001b[0m )\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/SKRIPSIII/PyABSAfarrel/pyabsa/framework/trainer_class/trainer_template.py:242\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mseed \u001b[38;5;241m=\u001b[39m s\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mcheckpoint_save_mode:\n\u001b[0;32m--> 242\u001b[0m     model_path\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_instructor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;66;03m# always return the last trained model if you don't save trained model\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference_model_class(\n\u001b[1;32m    246\u001b[0m         checkpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_instructor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig)\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m    247\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/SKRIPSIII/PyABSAfarrel/pyabsa/tasks/AspectTermExtraction/instructor/atepc_instructor.py:889\u001b[0m, in \u001b[0;36mATEPCTrainingInstructor.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    888\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 889\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/SKRIPSIII/PyABSAfarrel/pyabsa/framework/instructor_class/instructor_template.py:373\u001b[0m, in \u001b[0;36mBaseTrainingInstructor._train\u001b[0;34m(self, criterion)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_k_fold_train_and_evaluate(criterion)\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# Train and evaluate the model if there is only one validation dataloader\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 373\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/SKRIPSIII/PyABSAfarrel/pyabsa/tasks/AspectTermExtraction/instructor/atepc_instructor.py:370\u001b[0m, in \u001b[0;36mATEPCTrainingInstructor._train_and_evaluate\u001b[0;34m(self, criterion)\u001b[0m\n\u001b[1;32m    368\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m    369\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 370\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mwarmup_step \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/__init__.py:204\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/autograd/function.py:264\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBackwardCFunction\u001b[39;00m(_C\u001b[38;5;241m.\u001b[39m_FunctionBase, FunctionCtx, _HookMixin):\n\u001b[0;32m--> 264\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;66;03m# _forward_cls is defined by derived class\u001b[39;00m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;66;03m# The user should define either backward or vjp but never both.\u001b[39;00m\n\u001b[1;32m    267\u001b[0m         backward_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cls\u001b[38;5;241m.\u001b[39mbackward  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    268\u001b[0m         vjp_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_cls\u001b[38;5;241m.\u001b[39mvjp  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyabsa import ModelSaveOption, DeviceTypeOption\n",
    "import warnings\n",
    "\n",
    "#warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "config.num_epoch = 1\n",
    "# config.batch_size = 16\n",
    "# config.patience = 2\n",
    "# config.log_step = -1\n",
    "# config.seed = [1]\n",
    "# config.verbose = False  # If verbose == True, PyABSA will output the model strcture and seversal processed data examples\n",
    "config.scenario_case='manuallabelcheckpointoriginal'\n",
    "\n",
    "trainer = ATEPC.ATEPCTrainer(\n",
    "    config=config,\n",
    "    dataset=my_dataset,\n",
    "    from_checkpoint=\"/Users/dehan/Documents/SKRIPSIII/PyABSAfarrel/checkpoints/ENGLISH_ATE_TOURISM\",  # if you want to resume training from our pretrained checkpoints, you can pass the checkpoint name here\n",
    "    auto_device=DeviceTypeOption.AUTO,  # use cuda if available\n",
    "    checkpoint_save_mode=ModelSaveOption.SAVE_MODEL_STATE_DICT,  # save state dict only instead of the whole model\n",
    "    load_aug=False,  # there are some augmentation dataset for integrated datasets, you use them by setting load_aug=True to improve performance\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
