{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/bin/sh: nvidia-smi: command not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No CUDA GPU found in your device\n",
      "[2024-06-04 20:20:45] (2.3.1) \u001b[31mPyABSA(2.3.1): If your code crashes on Colab, please use the GPU runtime. Then run \"pip install pyabsa[dev] -U\" and restart the kernel.\n",
      "Or if it does not work, you can use v1.x versions, e.g., pip install pyabsa<2.0 -U\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "WARNING: When you fails to load a checkpoint, e.g., Unexpected key(s),\n",
      "Try to downgrade transformers<=4.29.0.\n",
      "\n",
      "\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dehan/opt/anaconda3/lib/python3.9/multiprocessing/pool.py:265: ResourceWarning: unclosed running multiprocessing pool <multiprocessing.pool.Pool state=RUN pool_size=1>\n",
      "  _warn(f\"unclosed running multiprocessing pool {self!r}\",\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "# Importing for pyabsa\n",
    "from pyabsa import AspectPolarityClassification as APC\n",
    "from pyabsa.tasks.AspectPolarityClassification.models import APCModelList    \n",
    "from pyabsa import ModelSaveOption, DeviceTypeOption    \n",
    "from pyabsa import DatasetItem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetItem(\"/Users/dehan/Documents/SKRIPSIII/PyABSA/integrated_datasets/apc_datasets\", \"132.manuallabel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-06-04 20:21:33] (2.3.1) Set Model Device: cpu\n",
      "[2024-06-04 20:21:33] (2.3.1) Device Name: Unknown\n",
      "2024-06-04 20:21:34,742 INFO: PyABSA version: 2.3.1\n",
      "2024-06-04 20:21:34,743 INFO: Transformers version: 4.29.0\n",
      "2024-06-04 20:21:34,744 INFO: Torch version: 2.1.0.dev20230313+cudaNone\n",
      "2024-06-04 20:21:34,744 INFO: Device: Unknown\n",
      "2024-06-04 20:21:34,745 INFO: Scenario Case: manuallabelcheckpointoriginal\n",
      "2024-06-04 20:21:34,745 INFO: apc_datasets in the trainer is not a exact path, will search dataset in current working directory\n",
      "2024-06-04 20:21:34,776 INFO: You can set load_aug=True in a trainer to augment your dataset (English only yet) and improve performance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dehan/opt/anaconda3/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:454: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at yangheng/deberta-v3-base-absa-v1.1 were not used when initializing DebertaV2Model: ['classifier.bias', 'classifier.weight', 'pooler.dense.weight', 'pooler.dense.bias']\n",
      "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-04 20:21:38,964 INFO: Load dataset from integrated_datasets/apc_datasets/132.manuallabel/train_set.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "preparing dataloader: 100%|██████████| 1076/1076 [00:00<00:00, 2169.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-04 20:21:39,464 INFO: Dataset Label Details: {'Neutral': 8, 'Positive': 598, 'Negative': 249, 'Sum': 855}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-04 20:21:39,724 INFO: train data examples:\n",
      " [{'ex_id': tensor(0), 'text_raw': '\"its entrance fee is cheap, but i quite regret it because there is a lot of people going in, but the main building is small... not much to see. the displays in the main building is quite dissappointing.. afterwards you can take a rest from your stroll at the park behind the main building..\"', 'text_spc': '[CLS] \"its entrance fee is cheap, but i quite regret it because there is a lot of people going in, but the main building is small... not much to see. the displays in the main building is quite dissappointing.. afterwards you can take a rest from your stroll at the park behind the main building..\" [SEP] people [SEP]', 'aspect': 'people', 'aspect_position': tensor(0), 'lca_ids': tensor([0.7714, 0.7857, 0.8000, 0.8143, 0.8286, 0.8429, 0.8571, 0.8714, 0.8857,\n",
      "        0.9000, 0.9143, 0.9286, 0.9429, 0.9571, 0.9714, 0.9857, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9857, 0.9714, 0.9571, 0.9429,\n",
      "        0.9286, 0.9143, 0.9000, 0.8857, 0.8714, 0.8571, 0.8429, 0.8286, 0.8143,\n",
      "        0.8000, 0.7857, 0.7714, 0.7571, 0.7429, 0.7286, 0.7143, 0.7000, 0.6857,\n",
      "        0.6714, 0.6571, 0.6429, 0.6286, 0.6143, 0.6000, 0.5857, 0.5714, 0.5571,\n",
      "        0.5429, 0.5286, 0.5143, 0.5000, 0.4857, 0.4714, 0.4571, 0.4429, 0.4286,\n",
      "        0.4143, 0.4000, 0.3857, 0.3714, 0.3571, 0.3429, 0.3286, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]), 'lcf_vec': tensor(0), 'lcf_cdw_vec': tensor([0.7714, 0.7857, 0.8000, 0.8143, 0.8286, 0.8429, 0.8571, 0.8714, 0.8857,\n",
      "        0.9000, 0.9143, 0.9286, 0.9429, 0.9571, 0.9714, 0.9857, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9857, 0.9714, 0.9571, 0.9429,\n",
      "        0.9286, 0.9143, 0.9000, 0.8857, 0.8714, 0.8571, 0.8429, 0.8286, 0.8143,\n",
      "        0.8000, 0.7857, 0.7714, 0.7571, 0.7429, 0.7286, 0.7143, 0.7000, 0.6857,\n",
      "        0.6714, 0.6571, 0.6429, 0.6286, 0.6143, 0.6000, 0.5857, 0.5714, 0.5571,\n",
      "        0.5429, 0.5286, 0.5143, 0.5000, 0.4857, 0.4714, 0.4571, 0.4429, 0.4286,\n",
      "        0.4143, 0.4000, 0.3857, 0.3714, 0.3571, 0.3429, 0.3286, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]), 'lcf_cdm_vec': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0]), 'lcfs_vec': tensor(0), 'lcfs_cdw_vec': tensor(0), 'lcfs_cdm_vec': tensor(0), 'dlcf_vec': tensor(0), 'dlcfs_vec': tensor(0), 'depend_vec': tensor(0), 'depended_vec': tensor(0), 'spc_mask_vec': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'text_indices': tensor([    1,   307, 11839,  4307,  2383,   269,  2315,   261,   304,   584,\n",
      "          817,  8186,   278,   401,   343,   269,   266,   509,   265,   355,\n",
      "          446,   267,   261,   304,   262,   872,   792,   269,   536,   260,\n",
      "          260,   260,   298,   400,   264,   398,   260,   262,  5309,   267,\n",
      "          262,   872,   792,   269,   817, 45890, 10256, 60271,   260,   260,\n",
      "         7574,   274,   295,   413,   266,  1090,   292,   290, 13046,   288,\n",
      "          262,  2010,   931,   262,   872,   792,   260,   260,   309,     2,\n",
      "          355,     2,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0]), 'aspect_bert_indices': tensor(0), 'text_raw_bert_indices': tensor(0), 'polarity': tensor(0), 'cluster_ids': tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100,    0, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100]), 'side_ex_ids': tensor(0), 'left_lcf_cdm_vec': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0]), 'left_lcf_cdw_vec': tensor([0.7714, 0.7857, 0.8000, 0.8143, 0.8286, 0.8429, 0.8571, 0.8714, 0.8857,\n",
      "        0.9000, 0.9143, 0.9286, 0.9429, 0.9571, 0.9714, 0.9857, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9857, 0.9714, 0.9571, 0.9429,\n",
      "        0.9286, 0.9143, 0.9000, 0.8857, 0.8714, 0.8571, 0.8429, 0.8286, 0.8143,\n",
      "        0.8000, 0.7857, 0.7714, 0.7571, 0.7429, 0.7286, 0.7143, 0.7000, 0.6857,\n",
      "        0.6714, 0.6571, 0.6429, 0.6286, 0.6143, 0.6000, 0.5857, 0.5714, 0.5571,\n",
      "        0.5429, 0.5286, 0.5143, 0.5000, 0.4857, 0.4714, 0.4571, 0.4429, 0.4286,\n",
      "        0.4143, 0.4000, 0.3857, 0.3714, 0.3571, 0.3429, 0.3286, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]), 'left_spc_mask_vec': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'left_text_indices': tensor([    1,   307, 11839,  4307,  2383,   269,  2315,   261,   304,   584,\n",
      "          817,  8186,   278,   401,   343,   269,   266,   509,   265,   355,\n",
      "          446,   267,   261,   304,   262,   872,   792,   269,   536,   260,\n",
      "          260,   260,   298,   400,   264,   398,   260,   262,  5309,   267,\n",
      "          262,   872,   792,   269,   817, 45890, 10256, 60271,   260,   260,\n",
      "         7574,   274,   295,   413,   266,  1090,   292,   290, 13046,   288,\n",
      "          262,  2010,   931,   262,   872,   792,   260,   260,   309,     2,\n",
      "          355,     2,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0]), 'left_dist': tensor(0), 'right_lcf_cdm_vec': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0]), 'right_lcf_cdw_vec': tensor([0.7714, 0.7857, 0.8000, 0.8143, 0.8286, 0.8429, 0.8571, 0.8714, 0.8857,\n",
      "        0.9000, 0.9143, 0.9286, 0.9429, 0.9571, 0.9714, 0.9857, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9857, 0.9714, 0.9571, 0.9429,\n",
      "        0.9286, 0.9143, 0.9000, 0.8857, 0.8714, 0.8571, 0.8429, 0.8286, 0.8143,\n",
      "        0.8000, 0.7857, 0.7714, 0.7571, 0.7429, 0.7286, 0.7143, 0.7000, 0.6857,\n",
      "        0.6714, 0.6571, 0.6429, 0.6286, 0.6143, 0.6000, 0.5857, 0.5714, 0.5571,\n",
      "        0.5429, 0.5286, 0.5143, 0.5000, 0.4857, 0.4714, 0.4571, 0.4429, 0.4286,\n",
      "        0.4143, 0.4000, 0.3857, 0.3714, 0.3571, 0.3429, 0.3286, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]), 'right_spc_mask_vec': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'right_text_indices': tensor([    1,   307, 11839,  4307,  2383,   269,  2315,   261,   304,   584,\n",
      "          817,  8186,   278,   401,   343,   269,   266,   509,   265,   355,\n",
      "          446,   267,   261,   304,   262,   872,   792,   269,   536,   260,\n",
      "          260,   260,   298,   400,   264,   398,   260,   262,  5309,   267,\n",
      "          262,   872,   792,   269,   817, 45890, 10256, 60271,   260,   260,\n",
      "         7574,   274,   295,   413,   266,  1090,   292,   290, 13046,   288,\n",
      "          262,  2010,   931,   262,   872,   792,   260,   260,   309,     2,\n",
      "          355,     2,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0]), 'right_dist': tensor(0)}, {'ex_id': tensor(1), 'text_raw': '\"tunjungan plaza mall is a shopper\\'s haven, offering an exceptional retail experience in surabaya.diverse shopping: this mall is a treasure trove of diverse stores catering to every need and desire. from high-end brands to local boutiques, the variety ensures there is something for everyone.ambiance and design: the mall\\'s design is impressive, blending modern architecture with a welcoming atmosphere. each section is well-lit and spacious, making shopping a comfortable and enjoyable affair.dining delights: beyond shopping, tunjungan plaza mall boasts an extensive array of dining options. from local delicacies to international cuisines, there is a culinary adventure awaiting every visitor.entertainment hub: it is not', 'text_spc': '[CLS] \"tunjungan plaza mall is a shopper\\'s haven, offering an exceptional retail experience in surabaya.diverse shopping: this mall is a treasure trove of diverse stores catering to every need and desire. from high-end brands to local boutiques, the variety ensures there is something for everyone.ambiance and design: the mall\\'s design is impressive, blending modern architecture with a welcoming atmosphere. each section is well-lit and spacious, making shopping a comfortable and enjoyable affair.dining delights: beyond shopping, tunjungan plaza mall boasts an extensive array of dining options. from local delicacies to international cuisines, there is a culinary adventure awaiting every visitor.entertainment hub: it is not [SEP] variety [SEP]', 'aspect': 'variety', 'aspect_position': tensor(0), 'lca_ids': tensor([0.5146, 0.5243, 0.5340, 0.5437, 0.5534, 0.5631, 0.5728, 0.5825, 0.5922,\n",
      "        0.6019, 0.6117, 0.6214, 0.6311, 0.6408, 0.6505, 0.6602, 0.6699, 0.6796,\n",
      "        0.6893, 0.6990, 0.7087, 0.7184, 0.7282, 0.7379, 0.7476, 0.7573, 0.7670,\n",
      "        0.7767, 0.7864, 0.7961, 0.8058, 0.8155, 0.8252, 0.8350, 0.8447, 0.8544,\n",
      "        0.8641, 0.8738, 0.8835, 0.8932, 0.9029, 0.9126, 0.9223, 0.9320, 0.9417,\n",
      "        0.9515, 0.9612, 0.9709, 0.9806, 0.9903, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 0.9903, 0.9806, 0.9709, 0.9612, 0.9515, 0.9417,\n",
      "        0.9320, 0.9223, 0.9126, 0.9029, 0.8932, 0.8835, 0.8738, 0.8641, 0.8544,\n",
      "        0.8447, 0.8350, 0.8252, 0.8155, 0.8058, 0.7961, 0.7864, 0.7767, 0.7670,\n",
      "        0.7573, 0.7476, 0.7379, 0.7282, 0.7184, 0.7087, 0.6990, 0.6893, 0.6796,\n",
      "        0.6699, 0.6602, 0.6505, 0.6408, 0.6311, 0.6214, 0.6117, 0.6019, 0.5922,\n",
      "        0.5825, 0.5728, 0.5631, 0.5534, 0.0000, 0.0000]), 'lcf_vec': tensor(0), 'lcf_cdw_vec': tensor([0.5146, 0.5243, 0.5340, 0.5437, 0.5534, 0.5631, 0.5728, 0.5825, 0.5922,\n",
      "        0.6019, 0.6117, 0.6214, 0.6311, 0.6408, 0.6505, 0.6602, 0.6699, 0.6796,\n",
      "        0.6893, 0.6990, 0.7087, 0.7184, 0.7282, 0.7379, 0.7476, 0.7573, 0.7670,\n",
      "        0.7767, 0.7864, 0.7961, 0.8058, 0.8155, 0.8252, 0.8350, 0.8447, 0.8544,\n",
      "        0.8641, 0.8738, 0.8835, 0.8932, 0.9029, 0.9126, 0.9223, 0.9320, 0.9417,\n",
      "        0.9515, 0.9612, 0.9709, 0.9806, 0.9903, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 0.9903, 0.9806, 0.9709, 0.9612, 0.9515, 0.9417,\n",
      "        0.9320, 0.9223, 0.9126, 0.9029, 0.8932, 0.8835, 0.8738, 0.8641, 0.8544,\n",
      "        0.8447, 0.8350, 0.8252, 0.8155, 0.8058, 0.7961, 0.7864, 0.7767, 0.7670,\n",
      "        0.7573, 0.7476, 0.7379, 0.7282, 0.7184, 0.7087, 0.6990, 0.6893, 0.6796,\n",
      "        0.6699, 0.6602, 0.6505, 0.6408, 0.6311, 0.6214, 0.6117, 0.6019, 0.5922,\n",
      "        0.5825, 0.5728, 0.5631, 0.5534, 0.0000, 0.0000]), 'lcf_cdm_vec': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0]), 'lcfs_vec': tensor(0), 'lcfs_cdw_vec': tensor(0), 'lcfs_cdm_vec': tensor(0), 'dlcf_vec': tensor(0), 'dlcfs_vec': tensor(0), 'depend_vec': tensor(0), 'depended_vec': tensor(0), 'spc_mask_vec': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'text_indices': tensor([    1,   307, 32414, 48432,  1398, 23989,  9267,   269,   266, 24041,\n",
      "          280,   268,  1791,   261,  1656,   299,  4551,  3035,   517,   267,\n",
      "        12271,   452, 64387,   260, 75595,  2017,   294,   291,  9267,   269,\n",
      "          266,  8422, 35611,   265,  3397,  2507, 10223,   264,   469,   389,\n",
      "          263,  2629,   260,   292,   459,   271,  3308,  3006,   264,   588,\n",
      "        28021,   261,   262,  1261,  6139,   343,   269,   491,   270,   837,\n",
      "          260, 35129,  8031,   263,   587,   294,   262,  9267,   280,   268,\n",
      "          587,   269,  3240,   261, 15610,  1252,  3781,   275,   266,  7743,\n",
      "         3384,   260,   448,  1317,   269,   371,   271, 13092,   263,  6779,\n",
      "          261,   570,  2017,   266,  1800,   263,  5896,  8083,   260, 32766,\n",
      "        22050,   294,  1579,  2017,   261]), 'aspect_bert_indices': tensor(0), 'text_raw_bert_indices': tensor(0), 'polarity': tensor(2), 'cluster_ids': tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100,    2, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100]), 'side_ex_ids': tensor(0), 'left_lcf_cdm_vec': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0]), 'left_lcf_cdw_vec': tensor([0.5146, 0.5243, 0.5340, 0.5437, 0.5534, 0.5631, 0.5728, 0.5825, 0.5922,\n",
      "        0.6019, 0.6117, 0.6214, 0.6311, 0.6408, 0.6505, 0.6602, 0.6699, 0.6796,\n",
      "        0.6893, 0.6990, 0.7087, 0.7184, 0.7282, 0.7379, 0.7476, 0.7573, 0.7670,\n",
      "        0.7767, 0.7864, 0.7961, 0.8058, 0.8155, 0.8252, 0.8350, 0.8447, 0.8544,\n",
      "        0.8641, 0.8738, 0.8835, 0.8932, 0.9029, 0.9126, 0.9223, 0.9320, 0.9417,\n",
      "        0.9515, 0.9612, 0.9709, 0.9806, 0.9903, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 0.9903, 0.9806, 0.9709, 0.9612, 0.9515, 0.9417,\n",
      "        0.9320, 0.9223, 0.9126, 0.9029, 0.8932, 0.8835, 0.8738, 0.8641, 0.8544,\n",
      "        0.8447, 0.8350, 0.8252, 0.8155, 0.8058, 0.7961, 0.7864, 0.7767, 0.7670,\n",
      "        0.7573, 0.7476, 0.7379, 0.7282, 0.7184, 0.7087, 0.6990, 0.6893, 0.6796,\n",
      "        0.6699, 0.6602, 0.6505, 0.6408, 0.6311, 0.6214, 0.6117, 0.6019, 0.5922,\n",
      "        0.5825, 0.5728, 0.5631, 0.5534, 0.0000, 0.0000]), 'left_spc_mask_vec': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'left_text_indices': tensor([    1,   307, 32414, 48432,  1398, 23989,  9267,   269,   266, 24041,\n",
      "          280,   268,  1791,   261,  1656,   299,  4551,  3035,   517,   267,\n",
      "        12271,   452, 64387,   260, 75595,  2017,   294,   291,  9267,   269,\n",
      "          266,  8422, 35611,   265,  3397,  2507, 10223,   264,   469,   389,\n",
      "          263,  2629,   260,   292,   459,   271,  3308,  3006,   264,   588,\n",
      "        28021,   261,   262,  1261,  6139,   343,   269,   491,   270,   837,\n",
      "          260, 35129,  8031,   263,   587,   294,   262,  9267,   280,   268,\n",
      "          587,   269,  3240,   261, 15610,  1252,  3781,   275,   266,  7743,\n",
      "         3384,   260,   448,  1317,   269,   371,   271, 13092,   263,  6779,\n",
      "          261,   570,  2017,   266,  1800,   263,  5896,  8083,   260, 32766,\n",
      "        22050,   294,  1579,  2017,   261]), 'left_dist': tensor(0), 'right_lcf_cdm_vec': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0]), 'right_lcf_cdw_vec': tensor([0.5146, 0.5243, 0.5340, 0.5437, 0.5534, 0.5631, 0.5728, 0.5825, 0.5922,\n",
      "        0.6019, 0.6117, 0.6214, 0.6311, 0.6408, 0.6505, 0.6602, 0.6699, 0.6796,\n",
      "        0.6893, 0.6990, 0.7087, 0.7184, 0.7282, 0.7379, 0.7476, 0.7573, 0.7670,\n",
      "        0.7767, 0.7864, 0.7961, 0.8058, 0.8155, 0.8252, 0.8350, 0.8447, 0.8544,\n",
      "        0.8641, 0.8738, 0.8835, 0.8932, 0.9029, 0.9126, 0.9223, 0.9320, 0.9417,\n",
      "        0.9515, 0.9612, 0.9709, 0.9806, 0.9903, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 0.9903, 0.9806, 0.9709, 0.9612, 0.9515, 0.9417,\n",
      "        0.9320, 0.9223, 0.9126, 0.9029, 0.8932, 0.8835, 0.8738, 0.8641, 0.8544,\n",
      "        0.8447, 0.8350, 0.8252, 0.8155, 0.8058, 0.7961, 0.7864, 0.7767, 0.7670,\n",
      "        0.7573, 0.7476, 0.7379, 0.7282, 0.7184, 0.7087, 0.6990, 0.6893, 0.6796,\n",
      "        0.6699, 0.6602, 0.6505, 0.6408, 0.6311, 0.6214, 0.6117, 0.6019, 0.5922,\n",
      "        0.5825, 0.5728, 0.5631, 0.5534, 0.0000, 0.0000]), 'right_spc_mask_vec': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'right_text_indices': tensor([    1,   307, 32414, 48432,  1398, 23989,  9267,   269,   266, 24041,\n",
      "          280,   268,  1791,   261,  1656,   299,  4551,  3035,   517,   267,\n",
      "        12271,   452, 64387,   260, 75595,  2017,   294,   291,  9267,   269,\n",
      "          266,  8422, 35611,   265,  3397,  2507, 10223,   264,   469,   389,\n",
      "          263,  2629,   260,   292,   459,   271,  3308,  3006,   264,   588,\n",
      "        28021,   261,   262,  1261,  6139,   343,   269,   491,   270,   837,\n",
      "          260, 35129,  8031,   263,   587,   294,   262,  9267,   280,   268,\n",
      "          587,   269,  3240,   261, 15610,  1252,  3781,   275,   266,  7743,\n",
      "         3384,   260,   448,  1317,   269,   371,   271, 13092,   263,  6779,\n",
      "          261,   570,  2017,   266,  1800,   263,  5896,  8083,   260, 32766,\n",
      "        22050,   294,  1579,  2017,   261]), 'right_dist': tensor(0)}]\n",
      "2024-06-04 20:21:39,918 INFO: Load dataset from integrated_datasets/apc_datasets/132.manuallabel/test_set.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "preparing dataloader: 100%|██████████| 359/359 [00:00<00:00, 2438.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-04 20:21:40,068 INFO: Dataset Label Details: {'Neutral': 4, 'Positive': 202, 'Negative': 84, 'Sum': 290}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-04 20:21:40,124 INFO: test data examples:\n",
      " [{'ex_id': tensor(0), 'text_raw': 'in 6 portions. incidentally we took a drop at no 1 and trust me it was awful. we entered in to food outlets and a few stores and wanted to go to the activity area and got the mall directory. went to the 5th floor in a conjested lift because only 1 was working and found out about the other parts of the mall. the wash room was pathetic with no water. came out and then went to part 6 which was a nice place and had the entrance toward s sheraton. if going there please get proper info and then go.', 'text_spc': '[CLS] in 6 portions. incidentally we took a drop at no 1 and trust me it was awful. we entered in to food outlets and a few stores and wanted to go to the activity area and got the mall directory. went to the 5th floor in a conjested lift because only 1 was working and found out about the other parts of the mall. the wash room was pathetic with no water. came out and then went to part 6 which was a nice place and had the entrance toward s sheraton. if going there please get proper info and then go. [SEP] part 6 [SEP]', 'aspect': 'part 6', 'aspect_position': tensor(0), 'lca_ids': tensor([0.1765, 0.1863, 0.1961, 0.2059, 0.2157, 0.2255, 0.2353, 0.2451, 0.2549,\n",
      "        0.2647, 0.2745, 0.2843, 0.2941, 0.3039, 0.3137, 0.3235, 0.3333, 0.3431,\n",
      "        0.3529, 0.3627, 0.3725, 0.3824, 0.3922, 0.4020, 0.4118, 0.4216, 0.4314,\n",
      "        0.4412, 0.4510, 0.4608, 0.4706, 0.4804, 0.4902, 0.5000, 0.5098, 0.5196,\n",
      "        0.5294, 0.5392, 0.5490, 0.5588, 0.5686, 0.5784, 0.5882, 0.5980, 0.6078,\n",
      "        0.6176, 0.6275, 0.6373, 0.6471, 0.6569, 0.6667, 0.6765, 0.6863, 0.6961,\n",
      "        0.7059, 0.7157, 0.7255, 0.7353, 0.7451, 0.7549, 0.7647, 0.7745, 0.7843,\n",
      "        0.7941, 0.8039, 0.8137, 0.8235, 0.8333, 0.8431, 0.8529, 0.8627, 0.8725,\n",
      "        0.8824, 0.8922, 0.9020, 0.9118, 0.9216, 0.9314, 0.9412, 0.9510, 0.9608,\n",
      "        0.9706, 0.9804, 0.9902, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 0.9902, 0.9804, 0.9706, 0.9608, 0.9510, 0.9412, 0.9314,\n",
      "        0.9216, 0.9118, 0.9020, 0.0000, 0.0000, 0.0000]), 'lcf_vec': tensor(0), 'lcf_cdw_vec': tensor([0.1765, 0.1863, 0.1961, 0.2059, 0.2157, 0.2255, 0.2353, 0.2451, 0.2549,\n",
      "        0.2647, 0.2745, 0.2843, 0.2941, 0.3039, 0.3137, 0.3235, 0.3333, 0.3431,\n",
      "        0.3529, 0.3627, 0.3725, 0.3824, 0.3922, 0.4020, 0.4118, 0.4216, 0.4314,\n",
      "        0.4412, 0.4510, 0.4608, 0.4706, 0.4804, 0.4902, 0.5000, 0.5098, 0.5196,\n",
      "        0.5294, 0.5392, 0.5490, 0.5588, 0.5686, 0.5784, 0.5882, 0.5980, 0.6078,\n",
      "        0.6176, 0.6275, 0.6373, 0.6471, 0.6569, 0.6667, 0.6765, 0.6863, 0.6961,\n",
      "        0.7059, 0.7157, 0.7255, 0.7353, 0.7451, 0.7549, 0.7647, 0.7745, 0.7843,\n",
      "        0.7941, 0.8039, 0.8137, 0.8235, 0.8333, 0.8431, 0.8529, 0.8627, 0.8725,\n",
      "        0.8824, 0.8922, 0.9020, 0.9118, 0.9216, 0.9314, 0.9412, 0.9510, 0.9608,\n",
      "        0.9706, 0.9804, 0.9902, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 0.9902, 0.9804, 0.9706, 0.9608, 0.9510, 0.9412, 0.9314,\n",
      "        0.9216, 0.9118, 0.9020, 0.0000, 0.0000, 0.0000]), 'lcf_cdm_vec': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0]), 'lcfs_vec': tensor(0), 'lcfs_cdw_vec': tensor(0), 'lcfs_cdm_vec': tensor(0), 'dlcf_vec': tensor(0), 'dlcfs_vec': tensor(0), 'depend_vec': tensor(0), 'depended_vec': tensor(0), 'spc_mask_vec': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'text_indices': tensor([    1,   267,   525,  9210,   260, 36552,   301,   681,   266,  1954,\n",
      "          288,   363,   376,   263,  1903,   351,   278,   284,  7630,   260,\n",
      "          301,  3040,   267,   264,   645,  7717,   263,   266,   477,  2507,\n",
      "          263,   849,   264,   424,   264,   262,  1506,   537,   263,   519,\n",
      "          262,  9267,  5514,   260,   700,   264,   262,   456,   474,  1397,\n",
      "          267,   266,  4636, 32707,  7043,  4168,   401,   364,   376,   284,\n",
      "          560,   263,   505,   321,   314,   262,   340,  1273,   265,   262,\n",
      "         9267,   260,   262,  4685,   629,   284, 21681,   275,   363,   529,\n",
      "          260,   670,   321,   263,   393,   700,   264,   465,   525,   319,\n",
      "          284,   266,  1085,   470,   263,   330,   262,  4307,  1955,  1550,\n",
      "          373,  3608,  2717,   260,   337]), 'aspect_bert_indices': tensor(0), 'text_raw_bert_indices': tensor(0), 'polarity': tensor(2), 'cluster_ids': tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100,    2,    2, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100]), 'side_ex_ids': tensor(0), 'left_lcf_cdm_vec': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0]), 'left_lcf_cdw_vec': tensor([0.1765, 0.1863, 0.1961, 0.2059, 0.2157, 0.2255, 0.2353, 0.2451, 0.2549,\n",
      "        0.2647, 0.2745, 0.2843, 0.2941, 0.3039, 0.3137, 0.3235, 0.3333, 0.3431,\n",
      "        0.3529, 0.3627, 0.3725, 0.3824, 0.3922, 0.4020, 0.4118, 0.4216, 0.4314,\n",
      "        0.4412, 0.4510, 0.4608, 0.4706, 0.4804, 0.4902, 0.5000, 0.5098, 0.5196,\n",
      "        0.5294, 0.5392, 0.5490, 0.5588, 0.5686, 0.5784, 0.5882, 0.5980, 0.6078,\n",
      "        0.6176, 0.6275, 0.6373, 0.6471, 0.6569, 0.6667, 0.6765, 0.6863, 0.6961,\n",
      "        0.7059, 0.7157, 0.7255, 0.7353, 0.7451, 0.7549, 0.7647, 0.7745, 0.7843,\n",
      "        0.7941, 0.8039, 0.8137, 0.8235, 0.8333, 0.8431, 0.8529, 0.8627, 0.8725,\n",
      "        0.8824, 0.8922, 0.9020, 0.9118, 0.9216, 0.9314, 0.9412, 0.9510, 0.9608,\n",
      "        0.9706, 0.9804, 0.9902, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 0.9902, 0.9804, 0.9706, 0.9608, 0.9510, 0.9412, 0.9314,\n",
      "        0.9216, 0.9118, 0.9020, 0.0000, 0.0000, 0.0000]), 'left_spc_mask_vec': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'left_text_indices': tensor([    1,   267,   525,  9210,   260, 36552,   301,   681,   266,  1954,\n",
      "          288,   363,   376,   263,  1903,   351,   278,   284,  7630,   260,\n",
      "          301,  3040,   267,   264,   645,  7717,   263,   266,   477,  2507,\n",
      "          263,   849,   264,   424,   264,   262,  1506,   537,   263,   519,\n",
      "          262,  9267,  5514,   260,   700,   264,   262,   456,   474,  1397,\n",
      "          267,   266,  4636, 32707,  7043,  4168,   401,   364,   376,   284,\n",
      "          560,   263,   505,   321,   314,   262,   340,  1273,   265,   262,\n",
      "         9267,   260,   262,  4685,   629,   284, 21681,   275,   363,   529,\n",
      "          260,   670,   321,   263,   393,   700,   264,   465,   525,   319,\n",
      "          284,   266,  1085,   470,   263,   330,   262,  4307,  1955,  1550,\n",
      "          373,  3608,  2717,   260,   337]), 'left_dist': tensor(0), 'right_lcf_cdm_vec': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0]), 'right_lcf_cdw_vec': tensor([0.1765, 0.1863, 0.1961, 0.2059, 0.2157, 0.2255, 0.2353, 0.2451, 0.2549,\n",
      "        0.2647, 0.2745, 0.2843, 0.2941, 0.3039, 0.3137, 0.3235, 0.3333, 0.3431,\n",
      "        0.3529, 0.3627, 0.3725, 0.3824, 0.3922, 0.4020, 0.4118, 0.4216, 0.4314,\n",
      "        0.4412, 0.4510, 0.4608, 0.4706, 0.4804, 0.4902, 0.5000, 0.5098, 0.5196,\n",
      "        0.5294, 0.5392, 0.5490, 0.5588, 0.5686, 0.5784, 0.5882, 0.5980, 0.6078,\n",
      "        0.6176, 0.6275, 0.6373, 0.6471, 0.6569, 0.6667, 0.6765, 0.6863, 0.6961,\n",
      "        0.7059, 0.7157, 0.7255, 0.7353, 0.7451, 0.7549, 0.7647, 0.7745, 0.7843,\n",
      "        0.7941, 0.8039, 0.8137, 0.8235, 0.8333, 0.8431, 0.8529, 0.8627, 0.8725,\n",
      "        0.8824, 0.8922, 0.9020, 0.9118, 0.9216, 0.9314, 0.9412, 0.9510, 0.9608,\n",
      "        0.9706, 0.9804, 0.9902, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 0.9902, 0.9804, 0.9706, 0.9608, 0.9510, 0.9412, 0.9314,\n",
      "        0.9216, 0.9118, 0.9020, 0.0000, 0.0000, 0.0000]), 'right_spc_mask_vec': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'right_text_indices': tensor([    1,   267,   525,  9210,   260, 36552,   301,   681,   266,  1954,\n",
      "          288,   363,   376,   263,  1903,   351,   278,   284,  7630,   260,\n",
      "          301,  3040,   267,   264,   645,  7717,   263,   266,   477,  2507,\n",
      "          263,   849,   264,   424,   264,   262,  1506,   537,   263,   519,\n",
      "          262,  9267,  5514,   260,   700,   264,   262,   456,   474,  1397,\n",
      "          267,   266,  4636, 32707,  7043,  4168,   401,   364,   376,   284,\n",
      "          560,   263,   505,   321,   314,   262,   340,  1273,   265,   262,\n",
      "         9267,   260,   262,  4685,   629,   284, 21681,   275,   363,   529,\n",
      "          260,   670,   321,   263,   393,   700,   264,   465,   525,   319,\n",
      "          284,   266,  1085,   470,   263,   330,   262,  4307,  1955,  1550,\n",
      "          373,  3608,  2717,   260,   337]), 'right_dist': tensor(0)}, {'ex_id': tensor(1), 'text_raw': 'stations, the government does not do a lot to promote the old batavia area. this is the old city part of north jakarta. neglected by most tourists visiting indonesia, you will mostly find local elementary school kids visiting the museum.which is a shame. i try to go there every month or so. ok, it can be annoying to be asked for autographs by hundreds of young kids (it seems to be a class assignment, just like \\'in-depth\\' interviews as \\'what do you like about indonesia?\\'). but the building makes up for it. and that for only a 2.000 rupiah entrance fee ..\"', 'text_spc': '[CLS] stations, the government does not do a lot to promote the old batavia area. this is the old city part of north jakarta. neglected by most tourists visiting indonesia, you will mostly find local elementary school kids visiting the museum.which is a shame. i try to go there every month or so. ok, it can be annoying to be asked for autographs by hundreds of young kids (it seems to be a class assignment, just like \\'in-depth\\' interviews as \\'what do you like about indonesia?\\'). but the building makes up for it. and that for only a 2.000 rupiah entrance fee ..\" [SEP] entrance fee [SEP]', 'aspect': 'entrance fee', 'aspect_position': tensor(0), 'lca_ids': tensor([-0.1961, -0.1863, -0.1765, -0.1667, -0.1569, -0.1471, -0.1373, -0.1275,\n",
      "        -0.1176, -0.1078, -0.0980, -0.0882, -0.0784, -0.0686, -0.0588, -0.0490,\n",
      "        -0.0392, -0.0294, -0.0196, -0.0098,  0.0000,  0.0098,  0.0196,  0.0294,\n",
      "         0.0392,  0.0490,  0.0588,  0.0686,  0.0784,  0.0882,  0.0980,  0.1078,\n",
      "         0.1176,  0.1275,  0.1373,  0.1471,  0.1569,  0.1667,  0.1765,  0.1863,\n",
      "         0.1961,  0.2059,  0.2157,  0.2255,  0.2353,  0.2451,  0.2549,  0.2647,\n",
      "         0.2745,  0.2843,  0.2941,  0.3039,  0.3137,  0.3235,  0.3333,  0.3431,\n",
      "         0.3529,  0.3627,  0.3725,  0.3824,  0.3922,  0.4020,  0.4118,  0.4216,\n",
      "         0.4314,  0.4412,  0.4510,  0.4608,  0.4706,  0.4804,  0.4902,  0.5000,\n",
      "         0.5098,  0.5196,  0.5294,  0.5392,  0.5490,  0.5588,  0.5686,  0.5784,\n",
      "         0.5882,  0.5980,  0.6078,  0.6176,  0.6275,  0.6373,  0.6471,  0.6569,\n",
      "         0.6667,  0.6765,  0.6863,  0.6961,  0.7059,  0.7157,  0.7255,  0.7353,\n",
      "         0.7451,  0.7549,  0.7647,  0.7745,  0.7843,  0.7941,  0.0000,  0.0000,\n",
      "         0.0000]), 'lcf_vec': tensor(0), 'lcf_cdw_vec': tensor([-0.1961, -0.1863, -0.1765, -0.1667, -0.1569, -0.1471, -0.1373, -0.1275,\n",
      "        -0.1176, -0.1078, -0.0980, -0.0882, -0.0784, -0.0686, -0.0588, -0.0490,\n",
      "        -0.0392, -0.0294, -0.0196, -0.0098,  0.0000,  0.0098,  0.0196,  0.0294,\n",
      "         0.0392,  0.0490,  0.0588,  0.0686,  0.0784,  0.0882,  0.0980,  0.1078,\n",
      "         0.1176,  0.1275,  0.1373,  0.1471,  0.1569,  0.1667,  0.1765,  0.1863,\n",
      "         0.1961,  0.2059,  0.2157,  0.2255,  0.2353,  0.2451,  0.2549,  0.2647,\n",
      "         0.2745,  0.2843,  0.2941,  0.3039,  0.3137,  0.3235,  0.3333,  0.3431,\n",
      "         0.3529,  0.3627,  0.3725,  0.3824,  0.3922,  0.4020,  0.4118,  0.4216,\n",
      "         0.4314,  0.4412,  0.4510,  0.4608,  0.4706,  0.4804,  0.4902,  0.5000,\n",
      "         0.5098,  0.5196,  0.5294,  0.5392,  0.5490,  0.5588,  0.5686,  0.5784,\n",
      "         0.5882,  0.5980,  0.6078,  0.6176,  0.6275,  0.6373,  0.6471,  0.6569,\n",
      "         0.6667,  0.6765,  0.6863,  0.6961,  0.7059,  0.7157,  0.7255,  0.7353,\n",
      "         0.7451,  0.7549,  0.7647,  0.7745,  0.7843,  0.7941,  0.0000,  0.0000,\n",
      "         0.0000]), 'lcf_cdm_vec': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0]), 'lcfs_vec': tensor(0), 'lcfs_cdw_vec': tensor(0), 'lcfs_cdm_vec': tensor(0), 'dlcf_vec': tensor(0), 'dlcfs_vec': tensor(0), 'depend_vec': tensor(0), 'depended_vec': tensor(0), 'spc_mask_vec': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'text_indices': tensor([    1,  4460,   261,   262,   671,   490,   298,   333,   266,   509,\n",
      "          264,  2655,   262,   597,  8006, 58867,   537,   260,   291,   269,\n",
      "          262,   597,   707,   465,   265,  2480, 77595, 68369,   260, 13272,\n",
      "          293,   370,  6662,  2868, 48415,   261,   274,   296,  2215,   433,\n",
      "          588,  8479,   563,   978,  2868,   262,  4419,   260,  2637,   269,\n",
      "          266,  6705,   260,   584,   687,   264,   424,   343,   469,   788,\n",
      "          289,   324,   260,  5522,   261,   278,   295,   282,  8106,   264,\n",
      "          282,   921,   270, 45217,   293,  2739,   265,   856,   978,   287,\n",
      "         1632,  1015,   264,   282,   266,   938,  6190,   261,   348,   334,\n",
      "          382,   547,   271,  7370,   280,  4973,   283,   382,  6356,   333,\n",
      "          274,   334,   314, 48415,   302]), 'aspect_bert_indices': tensor(0), 'text_raw_bert_indices': tensor(0), 'polarity': tensor(2), 'cluster_ids': tensor([-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100]), 'side_ex_ids': tensor(0), 'left_lcf_cdm_vec': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0]), 'left_lcf_cdw_vec': tensor([-0.1961, -0.1863, -0.1765, -0.1667, -0.1569, -0.1471, -0.1373, -0.1275,\n",
      "        -0.1176, -0.1078, -0.0980, -0.0882, -0.0784, -0.0686, -0.0588, -0.0490,\n",
      "        -0.0392, -0.0294, -0.0196, -0.0098,  0.0000,  0.0098,  0.0196,  0.0294,\n",
      "         0.0392,  0.0490,  0.0588,  0.0686,  0.0784,  0.0882,  0.0980,  0.1078,\n",
      "         0.1176,  0.1275,  0.1373,  0.1471,  0.1569,  0.1667,  0.1765,  0.1863,\n",
      "         0.1961,  0.2059,  0.2157,  0.2255,  0.2353,  0.2451,  0.2549,  0.2647,\n",
      "         0.2745,  0.2843,  0.2941,  0.3039,  0.3137,  0.3235,  0.3333,  0.3431,\n",
      "         0.3529,  0.3627,  0.3725,  0.3824,  0.3922,  0.4020,  0.4118,  0.4216,\n",
      "         0.4314,  0.4412,  0.4510,  0.4608,  0.4706,  0.4804,  0.4902,  0.5000,\n",
      "         0.5098,  0.5196,  0.5294,  0.5392,  0.5490,  0.5588,  0.5686,  0.5784,\n",
      "         0.5882,  0.5980,  0.6078,  0.6176,  0.6275,  0.6373,  0.6471,  0.6569,\n",
      "         0.6667,  0.6765,  0.6863,  0.6961,  0.7059,  0.7157,  0.7255,  0.7353,\n",
      "         0.7451,  0.7549,  0.7647,  0.7745,  0.7843,  0.7941,  0.0000,  0.0000,\n",
      "         0.0000]), 'left_spc_mask_vec': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'left_text_indices': tensor([    1,  4460,   261,   262,   671,   490,   298,   333,   266,   509,\n",
      "          264,  2655,   262,   597,  8006, 58867,   537,   260,   291,   269,\n",
      "          262,   597,   707,   465,   265,  2480, 77595, 68369,   260, 13272,\n",
      "          293,   370,  6662,  2868, 48415,   261,   274,   296,  2215,   433,\n",
      "          588,  8479,   563,   978,  2868,   262,  4419,   260,  2637,   269,\n",
      "          266,  6705,   260,   584,   687,   264,   424,   343,   469,   788,\n",
      "          289,   324,   260,  5522,   261,   278,   295,   282,  8106,   264,\n",
      "          282,   921,   270, 45217,   293,  2739,   265,   856,   978,   287,\n",
      "         1632,  1015,   264,   282,   266,   938,  6190,   261,   348,   334,\n",
      "          382,   547,   271,  7370,   280,  4973,   283,   382,  6356,   333,\n",
      "          274,   334,   314, 48415,   302]), 'left_dist': tensor(0), 'right_lcf_cdm_vec': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0]), 'right_lcf_cdw_vec': tensor([-0.1961, -0.1863, -0.1765, -0.1667, -0.1569, -0.1471, -0.1373, -0.1275,\n",
      "        -0.1176, -0.1078, -0.0980, -0.0882, -0.0784, -0.0686, -0.0588, -0.0490,\n",
      "        -0.0392, -0.0294, -0.0196, -0.0098,  0.0000,  0.0098,  0.0196,  0.0294,\n",
      "         0.0392,  0.0490,  0.0588,  0.0686,  0.0784,  0.0882,  0.0980,  0.1078,\n",
      "         0.1176,  0.1275,  0.1373,  0.1471,  0.1569,  0.1667,  0.1765,  0.1863,\n",
      "         0.1961,  0.2059,  0.2157,  0.2255,  0.2353,  0.2451,  0.2549,  0.2647,\n",
      "         0.2745,  0.2843,  0.2941,  0.3039,  0.3137,  0.3235,  0.3333,  0.3431,\n",
      "         0.3529,  0.3627,  0.3725,  0.3824,  0.3922,  0.4020,  0.4118,  0.4216,\n",
      "         0.4314,  0.4412,  0.4510,  0.4608,  0.4706,  0.4804,  0.4902,  0.5000,\n",
      "         0.5098,  0.5196,  0.5294,  0.5392,  0.5490,  0.5588,  0.5686,  0.5784,\n",
      "         0.5882,  0.5980,  0.6078,  0.6176,  0.6275,  0.6373,  0.6471,  0.6569,\n",
      "         0.6667,  0.6765,  0.6863,  0.6961,  0.7059,  0.7157,  0.7255,  0.7353,\n",
      "         0.7451,  0.7549,  0.7647,  0.7745,  0.7843,  0.7941,  0.0000,  0.0000,\n",
      "         0.0000]), 'right_spc_mask_vec': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'right_text_indices': tensor([    1,  4460,   261,   262,   671,   490,   298,   333,   266,   509,\n",
      "          264,  2655,   262,   597,  8006, 58867,   537,   260,   291,   269,\n",
      "          262,   597,   707,   465,   265,  2480, 77595, 68369,   260, 13272,\n",
      "          293,   370,  6662,  2868, 48415,   261,   274,   296,  2215,   433,\n",
      "          588,  8479,   563,   978,  2868,   262,  4419,   260,  2637,   269,\n",
      "          266,  6705,   260,   584,   687,   264,   424,   343,   469,   788,\n",
      "          289,   324,   260,  5522,   261,   278,   295,   282,  8106,   264,\n",
      "          282,   921,   270, 45217,   293,  2739,   265,   856,   978,   287,\n",
      "         1632,  1015,   264,   282,   266,   938,  6190,   261,   348,   334,\n",
      "          382,   547,   271,  7370,   280,  4973,   283,   382,  6356,   333,\n",
      "          274,   334,   314, 48415,   302]), 'right_dist': tensor(0)}]\n",
      "2024-06-04 20:21:40,125 INFO: valid data examples:\n",
      " []\n",
      "2024-06-04 20:21:40,168 INFO: Model Architecture:\n",
      " APCEnsembler(\n",
      "  (models): ModuleList(\n",
      "    (0): FAST_LSA_T_V2(\n",
      "      (bert4global): DebertaV2Model(\n",
      "        (embeddings): DebertaV2Embeddings(\n",
      "          (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "          (dropout): StableDropout()\n",
      "        )\n",
      "        (encoder): DebertaV2Encoder(\n",
      "          (layer): ModuleList(\n",
      "            (0-11): 12 x DebertaV2Layer(\n",
      "              (attention): DebertaV2Attention(\n",
      "                (self): DisentangledSelfAttention(\n",
      "                  (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (pos_dropout): StableDropout()\n",
      "                  (dropout): StableDropout()\n",
      "                )\n",
      "                (output): DebertaV2SelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "                  (dropout): StableDropout()\n",
      "                )\n",
      "              )\n",
      "              (intermediate): DebertaV2Intermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): DebertaV2Output(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "                (dropout): StableDropout()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (rel_embeddings): Embedding(512, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "      (post_encoder): Encoder(\n",
      "        (encoder): ModuleList(\n",
      "          (0): SelfAttention(\n",
      "            (SA): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (tanh): Tanh()\n",
      "      )\n",
      "      (post_encoder_): Encoder(\n",
      "        (encoder): ModuleList(\n",
      "          (0): SelfAttention(\n",
      "            (SA): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (tanh): Tanh()\n",
      "      )\n",
      "      (bert_pooler): BertPooler(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (activation): Tanh()\n",
      "      )\n",
      "      (CDW_LSA): LSA(\n",
      "        (encoder): Encoder(\n",
      "          (encoder): ModuleList(\n",
      "            (0): SelfAttention(\n",
      "              (SA): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (tanh): Tanh()\n",
      "        )\n",
      "        (encoder_left): Encoder(\n",
      "          (encoder): ModuleList(\n",
      "            (0): SelfAttention(\n",
      "              (SA): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (tanh): Tanh()\n",
      "        )\n",
      "        (encoder_right): Encoder(\n",
      "          (encoder): ModuleList(\n",
      "            (0): SelfAttention(\n",
      "              (SA): BertSelfAttention(\n",
      "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (tanh): Tanh()\n",
      "        )\n",
      "        (linear_window_3h): Linear(in_features=2304, out_features=768, bias=True)\n",
      "        (linear_window_2h): Linear(in_features=1536, out_features=768, bias=True)\n",
      "      )\n",
      "      (post_linear): Linear(in_features=1536, out_features=768, bias=True)\n",
      "      (dense): Linear(in_features=768, out_features=3, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (bert): DebertaV2Model(\n",
      "    (embeddings): DebertaV2Embeddings(\n",
      "      (word_embeddings): Embedding(128100, 768, padding_idx=0)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "      (dropout): StableDropout()\n",
      "    )\n",
      "    (encoder): DebertaV2Encoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x DebertaV2Layer(\n",
      "          (attention): DebertaV2Attention(\n",
      "            (self): DisentangledSelfAttention(\n",
      "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (pos_dropout): StableDropout()\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "            (output): DebertaV2SelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "              (dropout): StableDropout()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): DebertaV2Intermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): DebertaV2Output(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "            (dropout): StableDropout()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (rel_embeddings): Embedding(512, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (dense): Linear(in_features=3, out_features=3, bias=True)\n",
      ")\n",
      "2024-06-04 20:21:40,169 INFO: ABSADatasetsVersion:None\t-->\tCalling Count:0\n",
      "2024-06-04 20:21:40,170 INFO: MV:<metric_visualizer.metric_visualizer.MetricVisualizer object at 0x17df7ca30>\t-->\tCalling Count:0\n",
      "2024-06-04 20:21:40,171 INFO: PyABSAVersion:2.3.1\t-->\tCalling Count:1\n",
      "2024-06-04 20:21:40,173 INFO: SRD:3\t-->\tCalling Count:2290\n",
      "2024-06-04 20:21:40,174 INFO: TorchVersion:2.1.0.dev20230313+cudaNone\t-->\tCalling Count:1\n",
      "2024-06-04 20:21:40,175 INFO: TransformersVersion:4.29.0\t-->\tCalling Count:1\n",
      "2024-06-04 20:21:40,177 INFO: auto_device:cpu\t-->\tCalling Count:3\n",
      "2024-06-04 20:21:40,178 INFO: batch_size:16\t-->\tCalling Count:2\n",
      "2024-06-04 20:21:40,180 INFO: cache_dataset:False\t-->\tCalling Count:1\n",
      "2024-06-04 20:21:40,180 INFO: checkpoint_save_mode:1\t-->\tCalling Count:4\n",
      "2024-06-04 20:21:40,181 INFO: cross_validate_fold:-1\t-->\tCalling Count:1\n",
      "2024-06-04 20:21:40,181 INFO: dataset_file:{'train': ['integrated_datasets/apc_datasets/132.manuallabel/train_set.txt'], 'test': ['integrated_datasets/apc_datasets/132.manuallabel/test_set.txt'], 'valid': []}\t-->\tCalling Count:15\n",
      "2024-06-04 20:21:40,181 INFO: dataset_name:apc_datasets\t-->\tCalling Count:3\n",
      "2024-06-04 20:21:40,181 INFO: dca_layer:3\t-->\tCalling Count:0\n",
      "2024-06-04 20:21:40,182 INFO: dca_p:1\t-->\tCalling Count:0\n",
      "2024-06-04 20:21:40,182 INFO: deep_ensemble:False\t-->\tCalling Count:0\n",
      "2024-06-04 20:21:40,182 INFO: device:cpu\t-->\tCalling Count:3\n",
      "2024-06-04 20:21:40,183 INFO: device_name:Unknown\t-->\tCalling Count:1\n",
      "2024-06-04 20:21:40,183 INFO: dlcf_a:2\t-->\tCalling Count:0\n",
      "2024-06-04 20:21:40,183 INFO: dropout:0.5\t-->\tCalling Count:1\n",
      "2024-06-04 20:21:40,183 INFO: dynamic_truncate:True\t-->\tCalling Count:2290\n",
      "2024-06-04 20:21:40,184 INFO: embed_dim:768\t-->\tCalling Count:7\n",
      "2024-06-04 20:21:40,184 INFO: eta:1\t-->\tCalling Count:2\n",
      "2024-06-04 20:21:40,185 INFO: eta_lr:0.01\t-->\tCalling Count:1\n",
      "2024-06-04 20:21:40,187 INFO: evaluate_begin:0\t-->\tCalling Count:0\n",
      "2024-06-04 20:21:40,188 INFO: from_checkpoint:None\t-->\tCalling Count:0\n",
      "2024-06-04 20:21:40,189 INFO: hidden_dim:768\t-->\tCalling Count:0\n",
      "2024-06-04 20:21:40,189 INFO: index_to_label:{0: 'Negative', 1: 'Neutral', 2: 'Positive'}\t-->\tCalling Count:2\n",
      "2024-06-04 20:21:40,189 INFO: inference_model:None\t-->\tCalling Count:0\n",
      "2024-06-04 20:21:40,190 INFO: initializer:xavier_uniform_\t-->\tCalling Count:0\n",
      "2024-06-04 20:21:40,190 INFO: inputs_cols:['lcf_cdm_vec', 'lcf_cdw_vec', 'left_lcf_cdm_vec', 'left_lcf_cdw_vec', 'right_lcf_cdm_vec', 'right_lcf_cdw_vec', 'spc_mask_vec', 'text_indices']\t-->\tCalling Count:17180\n",
      "2024-06-04 20:21:40,191 INFO: l2reg:1e-05\t-->\tCalling Count:2\n",
      "2024-06-04 20:21:40,191 INFO: label_to_index:{'Negative': 0, 'Neutral': 1, 'Positive': 2}\t-->\tCalling Count:0\n",
      "2024-06-04 20:21:40,192 INFO: lcf:cdw\t-->\tCalling Count:3\n",
      "2024-06-04 20:21:40,192 INFO: learning_rate:2e-05\t-->\tCalling Count:1\n",
      "2024-06-04 20:21:40,192 INFO: load_aug:False\t-->\tCalling Count:1\n",
      "2024-06-04 20:21:40,192 INFO: log_step:5\t-->\tCalling Count:0\n",
      "2024-06-04 20:21:40,193 INFO: logger:<Logger fast_lsa_t_v2 (INFO)>\t-->\tCalling Count:15\n",
      "2024-06-04 20:21:40,193 INFO: lsa:True\t-->\tCalling Count:0\n",
      "2024-06-04 20:21:40,193 INFO: max_seq_len:105\t-->\tCalling Count:13740\n",
      "2024-06-04 20:21:40,193 INFO: model:<class 'pyabsa.tasks.AspectPolarityClassification.models.__lcf__.fast_lsa_t_v2.FAST_LSA_T_V2'>\t-->\tCalling Count:6\n",
      "2024-06-04 20:21:40,194 INFO: model_name:fast_lsa_t_v2\t-->\tCalling Count:2292\n",
      "2024-06-04 20:21:40,194 INFO: model_path_to_save:checkpoints\t-->\tCalling Count:0\n",
      "2024-06-04 20:21:40,194 INFO: num_epoch:1\t-->\tCalling Count:0\n",
      "2024-06-04 20:21:40,195 INFO: optimizer:adamw\t-->\tCalling Count:1\n",
      "2024-06-04 20:21:40,195 INFO: output_dim:3\t-->\tCalling Count:3\n",
      "2024-06-04 20:21:40,196 INFO: overwrite_cache:False\t-->\tCalling Count:0\n",
      "2024-06-04 20:21:40,196 INFO: path_to_save:None\t-->\tCalling Count:1\n",
      "2024-06-04 20:21:40,196 INFO: patience:99999\t-->\tCalling Count:0\n",
      "2024-06-04 20:21:40,196 INFO: pretrained_bert:yangheng/deberta-v3-base-absa-v1.1\t-->\tCalling Count:5\n",
      "2024-06-04 20:21:40,197 INFO: save_mode:1\t-->\tCalling Count:0\n",
      "2024-06-04 20:21:40,197 INFO: scenario_case:manuallabelcheckpointoriginal\t-->\tCalling Count:1\n",
      "2024-06-04 20:21:40,197 INFO: seed:52\t-->\tCalling Count:7\n",
      "2024-06-04 20:21:40,198 INFO: sigma:0.3\t-->\tCalling Count:0\n",
      "2024-06-04 20:21:40,198 INFO: similarity_threshold:1\t-->\tCalling Count:2\n",
      "2024-06-04 20:21:40,198 INFO: spacy_model:en_core_web_sm\t-->\tCalling Count:3\n",
      "2024-06-04 20:21:40,199 INFO: srd_alignment:True\t-->\tCalling Count:0\n",
      "2024-06-04 20:21:40,199 INFO: task_code:APC\t-->\tCalling Count:1\n",
      "2024-06-04 20:21:40,199 INFO: task_name:Aspect-based Sentiment Classification\t-->\tCalling Count:0\n",
      "2024-06-04 20:21:40,199 INFO: tokenizer:DebertaV2TokenizerFast(name_or_path='yangheng/deberta-v3-base-absa-v1.1', vocab_size=128000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)\t-->\tCalling Count:0\n",
      "2024-06-04 20:21:40,200 INFO: use_amp:False\t-->\tCalling Count:1\n",
      "2024-06-04 20:21:40,200 INFO: use_bert_spc:True\t-->\tCalling Count:0\n",
      "2024-06-04 20:21:40,200 INFO: use_syntax_based_SRD:False\t-->\tCalling Count:0\n",
      "2024-06-04 20:21:40,200 INFO: warmup_step:-1\t-->\tCalling Count:0\n",
      "2024-06-04 20:21:40,201 INFO: window:lr\t-->\tCalling Count:0\n",
      "2024-06-04 20:21:40,202 INFO: ***** Running training for Aspect-based Sentiment Classification *****\n",
      "2024-06-04 20:21:40,202 INFO: Training set examples = 855\n",
      "2024-06-04 20:21:40,203 INFO: Test set examples = 290\n",
      "2024-06-04 20:21:40,203 INFO: Total params = 197414417, Trainable params = 197414417, Non-trainable params = 0\n",
      "2024-06-04 20:21:40,203 INFO: Batch size = 16\n",
      "2024-06-04 20:21:40,204 INFO: Num steps = 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  1 | Smooth Loss: 0.7834:  17%|█▋        | 9/54 [00:45<03:46,  5.03s/it, Dev Acc:84.83(max:84.83) | Dev F1:53.12(max:53.12) | Precision:60.07 | Recall:51.03]\n",
      "[E thread_pool.cpp:109] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "[E thread_pool.cpp:109] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "[E thread_pool.cpp:109] Exception in thread pool task: mutex lock failed: Invalid argument\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m config\u001b[38;5;241m.\u001b[39mscenario_case\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmanuallabelcheckpointoriginal\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# try:\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mAPC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAPCTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#from_checkpoint='/Users/dehan/Documents/SKRIPSIII/PyABSA/checkpoints/ENGLISH_APC_TOURISM',  # if you want to resume training from our pretrained checkpoints, you can pass the checkpoint name here\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauto_device\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDeviceTypeOption\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCPU\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_to_save\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# set a path to save checkpoints, if it is None, save checkpoints at 'checkpoints' folder\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_save_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mModelSaveOption\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSAVE_MODEL_STATE_DICT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_aug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# there are some augmentation dataset for integrated datasets, you use them by setting load_aug=True to improve performance\u001b[39;49;00m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/SKRIPSIII/PyABSA/pyabsa/tasks/AspectPolarityClassification/trainer/apc_trainer.py:69\u001b[0m, in \u001b[0;36mAPCTrainer.__init__\u001b[0;34m(self, config, dataset, from_checkpoint, checkpoint_save_mode, auto_device, path_to_save, load_aug)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtask_code \u001b[38;5;241m=\u001b[39m TaskCodeOption\u001b[38;5;241m.\u001b[39mAspect_Polarity_Classification\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtask_name \u001b[38;5;241m=\u001b[39m TaskNameOption()\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m     66\u001b[0m     TaskCodeOption\u001b[38;5;241m.\u001b[39mAspect_Polarity_Classification\n\u001b[1;32m     67\u001b[0m )\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/SKRIPSIII/PyABSA/pyabsa/framework/trainer_class/trainer_template.py:242\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mseed \u001b[38;5;241m=\u001b[39m s\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mcheckpoint_save_mode:\n\u001b[0;32m--> 242\u001b[0m     model_path\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_instructor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    244\u001b[0m     \u001b[38;5;66;03m# always return the last trained model if you don't save trained model\u001b[39;00m\n\u001b[1;32m    245\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference_model_class(\n\u001b[1;32m    246\u001b[0m         checkpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_instructor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig)\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m    247\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/SKRIPSIII/PyABSA/pyabsa/tasks/AspectPolarityClassification/instructor/apc_instructor.py:897\u001b[0m, in \u001b[0;36mAPCTrainingInstructor.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    895\u001b[0m     \u001b[38;5;66;03m# Loss and Optimizer\u001b[39;00m\n\u001b[1;32m    896\u001b[0m     criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m--> 897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/SKRIPSIII/PyABSA/pyabsa/framework/instructor_class/instructor_template.py:373\u001b[0m, in \u001b[0;36mBaseTrainingInstructor._train\u001b[0;34m(self, criterion)\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_k_fold_train_and_evaluate(criterion)\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# Train and evaluate the model if there is only one validation dataloader\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 373\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/SKRIPSIII/PyABSA/pyabsa/tasks/AspectPolarityClassification/instructor/apc_instructor.py:166\u001b[0m, in \u001b[0;36mAPCTrainingInstructor._train_and_evaluate\u001b[0;34m(self, criterion)\u001b[0m\n\u001b[1;32m    162\u001b[0m     test_acc, f1, precision, recall \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluate_acc_f1(\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_dataloaders[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    164\u001b[0m     )\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 166\u001b[0m     test_acc, f1, precision, recall \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate_acc_f1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmetrics_of_this_checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m test_acc\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmetrics_of_this_checkpoint[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m f1\n",
      "File \u001b[0;32m~/Documents/SKRIPSIII/PyABSA/pyabsa/tasks/AspectPolarityClassification/instructor/apc_instructor.py:791\u001b[0m, in \u001b[0;36mAPCTrainingInstructor._evaluate_acc_f1\u001b[0;34m(self, test_dataloader)\u001b[0m\n\u001b[1;32m    784\u001b[0m t_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    785\u001b[0m     col: t_sample_batched[col]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39minputs_cols\n\u001b[1;32m    787\u001b[0m }\n\u001b[1;32m    789\u001b[0m t_targets \u001b[38;5;241m=\u001b[39m t_sample_batched[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpolarity\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 791\u001b[0m t_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t_outputs, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    794\u001b[0m     sen_outputs \u001b[38;5;241m=\u001b[39m t_outputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogits\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/SKRIPSIII/PyABSA/pyabsa/tasks/AspectPolarityClassification/instructor/ensembler.py:312\u001b[0m, in \u001b[0;36mAPCEnsembler.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m--> 312\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels[i](inputs) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels))]\n\u001b[1;32m    313\u001b[0m     loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.0\u001b[39m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensemble_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig:\n",
      "File \u001b[0;32m~/Documents/SKRIPSIII/PyABSA/pyabsa/tasks/AspectPolarityClassification/instructor/ensembler.py:312\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[0;32m--> 312\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels))]\n\u001b[1;32m    313\u001b[0m     loss \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m0.0\u001b[39m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensemble_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documents/SKRIPSIII/PyABSA/pyabsa/tasks/AspectPolarityClassification/models/__lcf__/fast_lsa_t_v2.py:58\u001b[0m, in \u001b[0;36mFAST_LSA_T_V2.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     55\u001b[0m left_lcf_cdm_matrix \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft_lcf_cdm_vec\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     56\u001b[0m right_lcf_cdm_matrix \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright_lcf_cdm_vec\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m global_context_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert4global\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_indices\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlast_hidden_state\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlcf \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcdw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     61\u001b[0m     sent_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCDW_LSA(\n\u001b[1;32m     62\u001b[0m         global_context_features,\n\u001b[1;32m     63\u001b[0m         spc_mask_vec\u001b[38;5;241m=\u001b[39mspc_mask_vec,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m         right_lcf_matrix\u001b[38;5;241m=\u001b[39mright_lcf_cdw_matrix,\n\u001b[1;32m     67\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:1083\u001b[0m, in \u001b[0;36mDebertaV2Model.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1073\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m   1075\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1076\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1077\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1080\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1081\u001b[0m )\n\u001b[0;32m-> 1083\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1089\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1090\u001b[0m encoded_layers \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:521\u001b[0m, in \u001b[0;36mDebertaV2Encoder.forward\u001b[0;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[1;32m    512\u001b[0m     output_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    513\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    514\u001b[0m         next_kv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    518\u001b[0m         rel_embeddings,\n\u001b[1;32m    519\u001b[0m     )\n\u001b[1;32m    520\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 521\u001b[0m     output_states \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnext_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    528\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    531\u001b[0m     output_states, att_m \u001b[38;5;241m=\u001b[39m output_states\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:372\u001b[0m, in \u001b[0;36mDebertaV2Layer.forward\u001b[0;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    371\u001b[0m     attention_output, att_matrix \u001b[38;5;241m=\u001b[39m attention_output\n\u001b[0;32m--> 372\u001b[0m intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    373\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:325\u001b[0m, in \u001b[0;36mDebertaV2Intermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    324\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[0;32m--> 325\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate_act_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/transformers/activations.py:78\u001b[0m, in \u001b[0;36mGELUActivation.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dataset = DatasetItem(\"/Users/dehan/Documents/SKRIPSIII/PyABSA/integrated_datasets/apc_datasets\", \"132.manuallabel\")\n",
    "\n",
    "# Load config model\n",
    "config = APC.APCConfigManager.get_apc_config_english()\n",
    "config.learning_rate = 2e-5\n",
    "config.model = APCModelList.FAST_LSA_T_V2\n",
    "#config.pretrained_bert = 'indobenchmark/indobert-base-p1'\n",
    "config.cache_dataset = False\n",
    "config.max_seq_len = 105\n",
    "config.num_epoch = 1\n",
    "config.lsa = True\n",
    "config.l2reg = 1e-5\n",
    "config.optimizer = 'adamw'\n",
    "config.eta_lr = 0.01\n",
    "config.batch_size = 16\n",
    "config.scenario_case='manuallabelcheckpointoriginal'\n",
    "# Train model\n",
    "# try:\n",
    "trainer = APC.APCTrainer(\n",
    "    config=config,\n",
    "    dataset=dataset,\n",
    "    #from_checkpoint='/Users/dehan/Documents/SKRIPSIII/PyABSA/checkpoints/ENGLISH_APC_TOURISM',  # if you want to resume training from our pretrained checkpoints, you can pass the checkpoint name here\n",
    "    auto_device=DeviceTypeOption.CPU,\n",
    "    path_to_save=None,\n",
    "        # set a path to save checkpoints, if it is None, save checkpoints at 'checkpoints' folder\n",
    "    checkpoint_save_mode=ModelSaveOption.SAVE_MODEL_STATE_DICT,\n",
    "    load_aug=False,\n",
    "        # there are some augmentation dataset for integrated datasets, you use them by setting load_aug=True to improve performance\n",
    "    )\n",
    "   \n",
    "# except Exception as e:\n",
    "#     raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.num_epoch = 1\n",
    "config.model = APC.APCModelList.FAST_LSA_T_V2\n",
    "trainer = APC.APCTrainer(\n",
    "    config=config,\n",
    "    dataset=dataset,\n",
    "    from_checkpoint=\"/Users/dehan/Documents/SKRIPSIII/PyABSA/checkpoints/ENGLISH_APC_TOURISM\",\n",
    "    # if you want to resume training from our pretrained checkpoints, you can pass the checkpoint name here\n",
    "    auto_device=DeviceTypeOption.AUTO,\n",
    "    path_to_save=None,  # set a path to save checkpoints, if it is None, save checkpoints at 'checkpoints' folder\n",
    "    checkpoint_save_mode=ModelSaveOption.SAVE_MODEL_STATE_DICT,\n",
    "    load_aug=False,\n",
    "    # there are some augmentation dataset for integrated datasets, you use them by setting load_aug=True to improve performance\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
